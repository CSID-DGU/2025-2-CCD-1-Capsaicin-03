from fastapi import FastAPI, File, Form, UploadFile, HTTPException
from pydantic import BaseModel
from pathlib import Path
import openai
import uuid
import shutil
import traceback
from datetime import datetime
from sel_characters import SEL_CHARACTERS
from sel_dialogue_generator import second_ai
from safety_filter import safety_filter

from sel_dialogue_generator import ask_experience

from sel_dialogue_generator import give_advice

from sel_dialogue_generator import action_card

import os


import subprocess
import tempfile

# âœ… ffmpeg ê²½ë¡œ ë¨¼ì € ì„¤ì •
ffmpeg_path = r"C:\ffmpeg\bin\ffmpeg.exe"
os.environ["PATH"] = r"C:\ffmpeg\bin;" + os.environ["PATH"]
os.environ["IMAGEIO_FFMPEG_EXE"] = ffmpeg_path

# âœ… whisperëŠ” PATH ì„¤ì • ì´í›„ì— import
import whisper


app = FastAPI()

@app.post("/api/conversations")
async def continue_conversation(
    story_name: str = Form(...),
    child_name: str = Form(...),
    ai_intro: str = Form(...),
    turn: int = Form(...),
    # child_text: str = Form(...)
    child_file: UploadFile = File(...)
):

    # Whisper ì‹¤í–‰
    model = whisper.load_model("medium")
    
    # 2ï¸âƒ£ ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ ê²½ë¡œì— ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_audio:
        temp_audio.write(await child_file.read())
        temp_audio_path = temp_audio.name

    # 3ï¸âƒ£ Whisperë¡œ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
    result = model.transcribe(temp_audio_path)
    child_text = result["text"]

    # 4ï¸âƒ£ ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.remove(temp_audio_path)
    
    # audio_path = f"{child_file}"
    # result = model.transcribe(audio_path)
    # print(result["text"])

    
    print(f"ë™í™”ì œëª©: {story_name}")
    character_name = SEL_CHARACTERS[story_name]['character_name']
    scene = SEL_CHARACTERS[story_name]['scene']
    print(f"ì¥ë©´: {scene}")
    print(f"{character_name}: {child_name + ' ì•„(ì•¼) ' + SEL_CHARACTERS[story_name]['intro']}")
    
    # intro = SEL_CHARACTERS[story_name]['intro']
    # ffmpeg ê²½ë¡œ ê°•ì œ ë“±ë¡
    ffmpeg_path = r"C:\ffmpeg\bin\ffmpeg.exe"
    os.environ["PATH"] = r"C:\ffmpeg\bin;" + os.environ["PATH"]
    os.environ["IMAGEIO_FFMPEG_EXE"] = ffmpeg_path

    # ffmpeg í™•ì¸
    # subprocess.run([ffmpeg_path, "-version"])
    # Whisperìš© ì„ì‹œ íŒŒì¼ ì €ì¥
    # with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_audio:
    #     contents = await child_file.read()
    #     temp_audio.write(contents)
    #     temp_audio_path = temp_audio.name  # WhisperëŠ” íŒŒì¼ ê²½ë¡œë¥¼
        
    # # Whisper ì‹¤í–‰
    # model = whisper.load_model("base")
    # result = model.transcribe(temp_audio_path)
    # child_text = result["test"]
    # print("ê²°ê³¼:", child_text)
    
    # # íŒŒì¼ ì‚­ì œ
    # os.remove(temp_audio_path)
    
    # if not child_text:
    #     raise ValueError(" ìŒì„± íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # print(f"ğŸ¤ ì¸ì‹ëœ í…ìŠ¤íŠ¸: {child_text}")

    if(turn==2):
        # child_text = input("ì•„ì´ì˜ ì²«ë²ˆì§¸ ëŒ€ë‹µ:").strip()
        unsafe = safety_filter(child_text)
        
        ai_text, emotion = second_ai(child_name, story_name, child_text)
        if unsafe:
            ai_text += "í•˜ì§€ë§Œ ê·¸ëŸ° ë§ í•˜ë©´ ì•ˆë¼~"           
        # ì•„ì´ê°€ ê°ì •ì„ ì¶”ì •í•œ ê²ƒì„ ë“£ê³  AIê°€ ê·¸ ì´ìœ ë¥¼ ë¬¼ì–´ë´„    
        print(ai_text)
    
    
    if(turn==3):
        # ì•„ì´ê°€ ì´ìœ ë¥¼ ë§í•¨
        # child_answer = input("ì•„ì´ì˜ ë‘ë²ˆì§¸ ëŒ€ë‹µ:").strip()
        print(child_text)
        # AIê°€ ì•„ì´ì—ê²Œ ê·¸ëŸ° ê°ì •ì´ ë“  ìœ ì‚¬í•œ ê²½í—˜ì„ ë¬¼ì–´ë´„
        ai_third_text = ask_experience(story_name, child_text);  
        print(ai_third_text)
    
    if(turn==4):
        # ì•„ì´ê°€ ìœ ì‚¬í•œ ê²½í—˜ì„ ë§í•¨
        # child_answer = input("ì•„ì´ì˜ ì„¸ë²ˆì§¸ ëŒ€ë‹µ:").strip()
        print(child_text)
        # êµ¬ì²´ì  í–‰ë™ ì „ëµ ì œì•ˆ(ì‹¬í˜¸í¡ 3ë²ˆ í•˜ê¸°) 
        # ex) ë‹¤ìŒì—ëŠ” ê·¸ëŸ° ì¼ ìˆìœ¼ë©´ ì‹¬í˜¸í¡ 3ë²ˆ í•´ë³¼ê¹Œ?
        ai_fourth_text = give_advice(story_name, child_text);  
        print(ai_fourth_text)
    
    # AIê°€ í–‰ë™ì¹´ë“œ ì œê³µ
    if(turn==5):
        # ì•„ì´ê°€ ì•Œê² ë‹¤/ì‹«ë‹¤ ëŒ€ë‹µ
        # child_answer = input("ì•„ì´ì˜ ë„¤ë²ˆì§¸ ëŒ€ë‹µ:").strip()
        print(child_text)
        # í–‰ë™ì¹´ë“œ ì œê³µ
        ai_fifth_text = action_card(story_name, child_text);  
        print(ai_fifth_text)


# if __name__ == "__main__":
#     continue_conversation() 